agent:
  gamma: 0.99                # Discount factor
  epsilon_start: 1.0               # Initial exploration probability
  epsilon_decay: 0.99995      # Decay rate of epsilon (matches paper)
  epsilon_min: 0.1           # Minimum epsilon for exploration
  learning_rate: 0.00025     # Learning rate for the optimizer (matches paper)
  target_update_frequency: 10000  # Update target network every 10,00 steps

replay_buffer:
  buffer_size: 1000000       # Increased to match original DQN
  batch_size: 32             # Matches paper

environment:
  game_name: "ALE/Pong-v5"   # Name of the Atari game
  render_mode: "rgb_array"   # Render mode (e.g., "human", "rgb_array", etc')
  obs_type: "grayscale"      # Observation type (e.g., "grayscale", "rgb", etc')
  max_steps: 10000           # Maximum steps per episode
  episodes: 500            # Increased episodes to match original training time
  batch_size: 32             # Matches paper
  target_update_frequency: 10000  # Update target network every 10,000 steps

checkpoints:
  load_checkpoint: False  # Enables resuming training
  checkpoint_path: "/home/ran/datasets/atari-mem/Pong-v5/20250315_215738/dqn_checkpoint_900.pth"  # Specify latest checkpoint
  frequency: 150
  data: 20

save_options: 
  folder: "/home/ran/datasets/atari-mem/Pong-v5"
  format: "csv"